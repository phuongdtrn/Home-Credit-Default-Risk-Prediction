{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61a9de8",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV, KFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e959e1b",
   "metadata": {},
   "source": [
    "Importing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('application_train.csv')\n",
    "test_df = pd.read_csv('application_test.csv')\n",
    "bureau = pd.read_csv('bureau.csv')\n",
    "pos = pd.read_csv('POS_CASH_balance.csv')\n",
    "credit_bal = pd.read_csv('credit_card_balance.csv')\n",
    "prev_df = pd.read_csv('previous_application.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325097c",
   "metadata": {},
   "source": [
    "### Feature Engineering  \n",
    "This is important as we want to be able to incorporate additional (meaningful) information to help our model's classification capabilities. Initial exploration of using only application_train and application_test which contained only basic information on the clients yielded very poor results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35642ad",
   "metadata": {},
   "source": [
    "Feature Engineering for Bureau  \n",
    "\n",
    "credit_overdue : To determine if there’s risk of overdue payments based on applicant history  \n",
    "debt_credit_ratio : Flag for at-risk customers that have incurred debt over credit limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_new = bureau[['SK_ID_CURR', 'CREDIT_DAY_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT']].copy()\n",
    "bureau_new['CREDIT_OVERDUE'] = bureau_new['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "bureau_new['DEBT_CREDIT_RATIO'] = bureau_new['AMT_CREDIT_SUM_DEBT'] / bureau_new['AMT_CREDIT_SUM']\n",
    "bureau_new = bureau_new.drop(['CREDIT_DAY_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT'], axis=1)\n",
    "bureau_new.fillna(0, inplace=True)\n",
    "bureau_new = bureau_new.groupby('SK_ID_CURR').mean().reset_index()\n",
    "bureau_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdab4e8",
   "metadata": {},
   "source": [
    "Feature Engineering for POS Cash Balance  \n",
    "\n",
    "cnt_installment_future : balance number of installments to pay for previous credit  \n",
    "sk_dpd_def : days past due during the month with tolerance for loans with low amount; possible flag for inability to pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a408b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_new = pos[['SK_ID_CURR', 'CNT_INSTALMENT_FUTURE', 'SK_DPD_DEF']].copy()\n",
    "pos_new = pos_new.groupby('SK_ID_CURR').mean().reset_index()\n",
    "pos_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36282f",
   "metadata": {},
   "source": [
    "Feature Engineering for Credit Card Balance  \n",
    "\n",
    "avg_vs_min_pay : Can be determinant of the applicant’s ability to pay above minimum requirements  \n",
    "amg_balance : credit balance  \n",
    "amt_credit_limit_actual : credit limit; a higher credit limit could indicate better client financial capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60eee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_bal_new = credit_bal[['SK_ID_CURR', 'AMT_BALANCE',\n",
    "                             'AMT_CREDIT_LIMIT_ACTUAL',\n",
    "                             'AMT_INST_MIN_REGULARITY',\n",
    "                             'AMT_PAYMENT_CURRENT',\n",
    "                             'AMT_PAYMENT_TOTAL_CURRENT']].copy()\n",
    "credit_bal_new['AVG_PAYMENTS'] = credit_bal_new['AMT_PAYMENT_TOTAL_CURRENT'] / (credit_bal_new['AMT_PAYMENT_TOTAL_CURRENT'] / credit_bal_new['AMT_PAYMENT_CURRENT'])\n",
    "credit_bal_new['AVG_VS_MIN_PAY'] = credit_bal_new['AVG_PAYMENTS'] / credit_bal_new['AMT_INST_MIN_REGULARITY']\n",
    "credit_bal_new = credit_bal_new.drop(['AMT_INST_MIN_REGULARITY',\n",
    "                                      'AMT_PAYMENT_CURRENT',\n",
    "                                      'AMT_PAYMENT_TOTAL_CURRENT',\n",
    "                                      'AVG_PAYMENTS'], axis=1)\n",
    "credit_bal_new.fillna(0, inplace=True)\n",
    "credit_bal_new = credit_bal_new.groupby('SK_ID_CURR').mean().reset_index()\n",
    "credit_bal_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c1d16",
   "metadata": {},
   "source": [
    "Feature Engineering for Previous Applications  \n",
    "\n",
    "approval_rate : Can be determinant of successful applications in the past  \n",
    "refusal_rate : Can be determinant of rejected applications in the past  \n",
    "avg_days_bet_rejection : How recent was the latest application rejection (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f07010",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_loan_status = prev_df[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].copy()\n",
    "dum_df = pd.get_dummies(prev_loan_status, columns=[\"NAME_CONTRACT_STATUS\"], prefix=[\"Type_is\"])\n",
    "prev_loan_stat = dum_df.groupby('SK_ID_CURR').sum().reset_index()\n",
    "\n",
    "prev_loan_stat['Approval_Rate'] = prev_loan_stat['Type_is_Approved'] / (prev_loan_stat['Type_is_Approved'] +\n",
    "                                                                        prev_loan_stat['Type_is_Refused'] +\n",
    "                                                                        prev_loan_stat['Type_is_Canceled'])\n",
    "prev_loan_stat['Refusal_Rate'] = prev_loan_stat['Type_is_Refused'] / (prev_loan_stat['Type_is_Approved'] +\n",
    "                                                                      prev_loan_stat['Type_is_Refused'] +\n",
    "                                                                      prev_loan_stat['Type_is_Canceled'])\n",
    "\n",
    "prev_application_days = prev_df[['SK_ID_CURR','DAYS_DECISION']].copy()\n",
    "prev_application_days_sum = prev_application_days.groupby('SK_ID_CURR').sum().reset_index()\n",
    "\n",
    "\n",
    "prev_combined_df = prev_loan_stat.merge(prev_application_days_sum, how='left', on='SK_ID_CURR')\n",
    "prev_combined_df['DAYS_DECISION'] = prev_combined_df['DAYS_DECISION']*-1\n",
    "\n",
    "prev_combined_df['avg_days_bet_rejection'] = prev_combined_df['DAYS_DECISION'] / prev_combined_df['Type_is_Refused']\n",
    "\n",
    "prev_combined_df.fillna(0, inplace=True)\n",
    "prev_combined_df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "prev_combined_df = prev_combined_df.drop(['Type_is_Approved',\n",
    "                                          'Type_is_Canceled',\n",
    "                                          'Type_is_Refused',\n",
    "                                          'Type_is_Unused offer',\n",
    "                                          'DAYS_DECISION'], axis=1)\n",
    "\n",
    "prev_combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dcca67",
   "metadata": {},
   "source": [
    "Combining with application_train and application_test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data set\n",
    "train_df = train_df.merge(bureau_new, how='left', on='SK_ID_CURR')\n",
    "train_df = train_df.merge(pos_new, how='left', on='SK_ID_CURR')\n",
    "train_df = train_df.merge(credit_bal_new, how='left', on='SK_ID_CURR')\n",
    "train_df = train_df.merge(prev_combined_df, how='left', on='SK_ID_CURR')\n",
    "\n",
    "# test data set\n",
    "test_df = test_df.merge(bureau_new, how='left', on='SK_ID_CURR')\n",
    "test_df = test_df.merge(pos_new, how='left', on='SK_ID_CURR')\n",
    "test_df = test_df.merge(credit_bal_new, how='left', on='SK_ID_CURR')\n",
    "test_df = test_df.merge(prev_combined_df, how='left', on='SK_ID_CURR')\n",
    "\n",
    "train_df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "test_df.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a5945",
   "metadata": {},
   "source": [
    "Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de619b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling missing values\n",
    "list_null_cols=train_df.isnull().sum()[train_df.isnull().sum()>0].index\n",
    "list_null_cols\n",
    "for i in list_null_cols:\n",
    "    if(train_df[i].dtype=='float64'):\n",
    "        train_df.loc[train_df[i].isnull(), i]=0.0\n",
    "    if(train_df[i].dtype=='int'):\n",
    "        train_df.loc[train_df[i].isnull(), i]=0\n",
    "    if(train_df[i].dtype=='object'):\n",
    "        train_df.loc[train_df[i].isnull(), i]='N/A'\n",
    "\n",
    "\n",
    "list_null_cols=test_df.isnull().sum()[test_df.isnull().sum()>0].index\n",
    "list_null_cols\n",
    "for i in list_null_cols:\n",
    "    if(test_df[i].dtype=='float64'):\n",
    "        test_df.loc[test_df[i].isnull(), i]=0.0\n",
    "    if(test_df[i].dtype=='int'):\n",
    "        test_df.loc[test_df[i].isnull(), i]=0\n",
    "    if(test_df[i].dtype=='object'):\n",
    "        test_df.loc[test_df[i].isnull(), i]='N/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4080a7",
   "metadata": {},
   "source": [
    "Splitting Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6945d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and labels for train dataset\n",
    "X_train = train_df.drop(['TARGET'], axis=1)\n",
    "y_train = train_df['TARGET'].copy()\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d52c0",
   "metadata": {},
   "source": [
    "Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns\n",
    "str_type = []\n",
    "for i in cols:\n",
    "  if(X_train[i].dtype=='object' or X_train[i].dtype=='str'):\n",
    "    str_type.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b779b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "for i in str_type:\n",
    "  X_train[i] = labelencoder.fit_transform(X_train[i].astype(str))\n",
    "  X_test[i] = labelencoder.fit_transform(X_test[i].astype(str))\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f64701",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.drop('SK_ID_CURR', axis=1)\n",
    "X_test2 = X_test.drop('SK_ID_CURR', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f082df",
   "metadata": {},
   "source": [
    "Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda48309",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "X_train_std = std_scale.fit_transform(X_train2)\n",
    "X_test_std = std_scale.transform(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c315d4",
   "metadata": {},
   "source": [
    "### MODELS  \n",
    "Below we test different learning models and determine best hyper parameter settings for each in order to maximize the AUC score. For each individual model run, the scores were noted upon the upload of test data set prediction probabilities to Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c43882",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv= KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4b500",
   "metadata": {},
   "source": [
    "#### Logistic Regression  \n",
    "SCORE: 0.717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87333e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid = {'lr__C': [0.01, 0.1, 1, 100, 1000],\n",
    "           'lr__class_weight': [None, 'balanced']}\n",
    "\n",
    "for i in range(3):\n",
    "    inner_cv = KFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    outer_cv = KFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    \n",
    "    lr_clf = RandomizedSearchCV(estimator=lr_pipeline, param_distributions=lr_grid,\n",
    "                                scoring='roc_auc', cv=inner_cv, n_jobs=-1)\n",
    "    \n",
    "    lr_score = cross_val_score(lr_clf, X=X_train_drp, y=y_train,\n",
    "                                cv=outer_cv, scoring='roc_auc',\n",
    "                                error_score=\"raise\")\n",
    "    \n",
    "\n",
    "# determining the best parameter settings\n",
    "lr_clf.fit(X_train_std, y_train)\n",
    "print(\"Best Parameter Settings: \",lr_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebc0fa",
   "metadata": {},
   "source": [
    "#### XGBoost  \n",
    "SCORE: 0.761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = {'xgb__gamma': [0.1, 1, 10],\n",
    "            'xgb__max_depth':range(2,17,5)\n",
    "            'xgb__n_estimators':range(200,1700,500)}\n",
    "        \n",
    "for i in range(3):\n",
    "    inner_cv = KFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    outer_cv = KFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    \n",
    "    xgb_clf = RandomizedSearchCV(estimator=xgb_pipeline, param_distributions=xgb_grid,\n",
    "                           scoring='roc_auc', cv=inner_cv, n_jobs=-1)\n",
    "    \n",
    "    xgb_score = cross_val_score(xgb_clf, X=X_train_drp, y=y_train,\n",
    "                                cv=outer_cv, scoring='roc_auc',\n",
    "                                error_score=\"raise\")\n",
    "    print(xgb_score)\n",
    "\n",
    "# determining best parameter settings\n",
    "xgb_clf.fit(X_train_std, y_train)\n",
    "print(\"Best Parameter Settings: \",xgb_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da88c1",
   "metadata": {},
   "source": [
    "#### LGBM  \n",
    "SCORE: 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3, X_val, y_train3, y_val = train_test_split(X_train2, y_train, test_size=0.30, random_state=42)\n",
    "X_train3_std = std_scale.fit_transform(X_train3)\n",
    "X_val_std = std_scale.transform(X_val)\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    nthread=4,\n",
    "    \n",
    "    colsample_bytree=0.9,\n",
    "    subsample=0.8,\n",
    "    max_depth=8,\n",
    "    reg_alpha=0.04,\n",
    "    reg_lambda=0.07,\n",
    "    min_split_gain=0.04,\n",
    "    min_child_weight=39,\n",
    "    silent=-1,\n",
    "    verbose=-1\n",
    "   )\n",
    "\n",
    "lgbm_grid={'boosting_type':['gbdt','goss'],\n",
    "           'n_estimators':[10000, 15000,20000],\n",
    "           'learning_rate':[0.0005, 0.001,0.01,0.05],\n",
    "           'num_leaves':[20,30,40,50]}\n",
    "\n",
    "lgbm_cl=GridSearchCV(lgbm, param_grid=lgbm_grid, cv = cv, scoring='roc_auc', n_jobs=-1)\n",
    "lgbm_cl.fit(X_train3_std,y_train3, eval_set=[(X_val_std,y_val)], verbose=200)\n",
    "lgbm_cl.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20a2ac",
   "metadata": {},
   "source": [
    "#### Random Forest  \n",
    "SCORE: 0.652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 100)]\n",
    "max_depth = [int(x) for x in np.linspace(5, 15, num = 2)]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "rf_grid = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_cl = RandomizedSearchCV(estimator = rf, param_distributions = rf_grid, cv = cv, verbose=2, random_state=42, n_jobs = -1, scoring = 'roc_auc')\n",
    "rf_cl.fit(X_train_std, y_train.values.ravel())\n",
    "rf_cl.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18aff10",
   "metadata": {},
   "source": [
    "#### AdaBoost  \n",
    "SCORE: 0.668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_grid = {'n_estimators': [50, 100, 150, 200, 300, 500],\n",
    "            'learning_rate': [0.5, 1.0, 1.5],\n",
    "            'algorithm': ['SAMME', 'SAMME.R']}\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "ada_clf = RandomizedSearchCV(estimator = ada, param_distributions = ada_grid, cv = cv, verbose=2, random_state=42, n_jobs = -1, scoring = 'roc_auc')\n",
    "ada_clf.fit(X_train_std, y_train.values.ravel())\n",
    "ada_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effe20f7",
   "metadata": {},
   "source": [
    "### Stacking  \n",
    "In an attempt to further increase the scores we obtained by running individual models, we employed the stacking technique which combined the all previous models in their best hyper parameter settings.  \n",
    "SCORE: 0.764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=507, max_depth=15,min_samples_leaf=4,min_samples_split=5,\n",
    "                       random_state=42)\n",
    "lr = LogisticRegression(class_weight=\"balanced\",C = 100, solver='liblinear')\n",
    "ada = AdaBoostClassifier(n_estimators=500,learning_rate=1.5,algorithm='SAMME')\n",
    "lgbm = LGBMClassifier(\n",
    "    nthread=4,\n",
    "    colsample_bytree=0.9,\n",
    "    subsample=0.8,\n",
    "    max_depth=8,\n",
    "    reg_alpha=0.04,\n",
    "    reg_lambda=0.07,\n",
    "    min_split_gain=0.04,\n",
    "    min_child_weight=39,\n",
    "    boosting_type='goss',\n",
    "    n_estimators=15000,\n",
    "    learning_rate=0.001,\n",
    "    num_leaves=30\n",
    "    )\n",
    "\n",
    "xgb_c = xgb.XGBClassifier(n_estimators=700,max_depth=2,gamma=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('xgb',xgb_c),         \n",
    "    ('rf', rf),\n",
    "    ('lr', lr),\n",
    "    ('ada',ada),\n",
    "    ('lgbm' , lgbm) ]\n",
    "clf = StackingClassifier(\n",
    "     estimators=estimators, final_estimator=xgb_c,\n",
    "        passthrough=True, verbose=2)\n",
    "\n",
    "clf.fit(X_train_std,y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f993e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['SK_ID_CURR']=test_df['SK_ID_CURR']\n",
    "df['TARGET']= out[:,1]\n",
    "df.to_csv('output3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650895ab",
   "metadata": {},
   "source": [
    "### Other Initial Explorations - but incorrect approach which we did not pursue further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5023ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef,cohen_kappa_score,make_scorer\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV,KFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix,confusion_matrix, ConfusionMatrixDisplay,roc_curve, auc, precision_recall_curve, recall_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "app_train= pd.read_csv('application_train.csv')\n",
    "app_train.head(n=5)\n",
    "\n",
    "list_null_cols=app_test.isnull().sum()[app_test.isnull().sum()>0].index\n",
    "list_null_cols\n",
    "for i in list_null_cols:\n",
    "    if(app_test[i].dtype=='float64'):\n",
    "        app_test.loc[app_test[i].isnull(), i]=0.0\n",
    "    if(app_test[i].dtype=='int'):\n",
    "        app_test.loc[app_test[i].isnull(), i]=0\n",
    "    if(app_test[i].dtype=='object'):\n",
    "        app_test.loc[app_test[i].isnull(), i]='N/A'\n",
    "        \n",
    "        \n",
    "X_train=app_train.loc[:, app_train.columns != 'TARGET']\n",
    "y_train=app_train.loc[:, app_train.columns == 'TARGET']\n",
    "X_test=app_test.loc[:, app_test.columns != 'TARGET']\n",
    "\n",
    "\n",
    "\n",
    "cols= X_train.columns\n",
    "string_type=[]\n",
    "for i in cols:\n",
    "    if X_train[i].dtype=='object':\n",
    "        string_type.append(i)\n",
    "#print(string_type)\n",
    "\n",
    "other_type=[]\n",
    "for i in cols:\n",
    "    if i not in string_type:\n",
    "        other_type.append(i)\n",
    "#print(other_type)\n",
    "\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc.fit(X_train[string_type])\n",
    "enc.fit(X_test[string_type])\n",
    "\n",
    "dummy1=enc.transform(X_train[string_type]).toarray()\n",
    "dummy=pd.DataFrame(dummy1,columns=enc.get_feature_names(string_type))\n",
    "non_dummy=X_train[other_type]\n",
    "X_train_trans=pd.concat([dummy, non_dummy], axis=1)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(4)\n",
    "X_train_trans2 = pca.fit_transform(X_train_trans)\n",
    "X_test_trans = pca.transform(X_test_trans)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inner_cv= KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv= KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "minmax = MinMaxScaler()\n",
    "std = StandardScaler()\n",
    "#scoring = make_scorer(matthews_corrcoef)\n",
    "#scoring = make_scorer(cohen_kappa_score)\n",
    "#scoring = make_scorer(f1_score)\n",
    "scoring = 'roc_auc'\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "lr = LogisticRegression()\n",
    "svm =SVC()\n",
    "dt=DecisionTreeClassifier()\n",
    "rf=RandomForestClassifier()\n",
    "xg = XGBClassifier()\n",
    "   \n",
    "\n",
    "            \n",
    "knn_grid={ 'n_neighbors': range(1,5,1)}\n",
    "\n",
    "lr_grid= { 'solver': ['newton-cg', 'lbfgs'],\n",
    "                        'penalty': ['l1', 'l2', 'elasticnet'], #['l1', 'l2', 'elasticnet']\n",
    "                        'C': [ .1,1.0 ]\n",
    "                        'l1_ratio': [0,0.5,1]\n",
    "         }\n",
    "\n",
    "svm_grid= {'kernel': [ 'rbf'], 'gamma': [1e-3, 1e-4]},{'kernel': [ 'linear'], 'C': [0.1,1, 10]}\n",
    "        \n",
    "\n",
    "dt_grid = {'max_depth': range(1,3)\n",
    "           'criterion': ['mse', 'mae'],\n",
    "           'min_samples_split' : range(2,5),\n",
    "           'min_samples_leaf' : range(2,5)\n",
    "        }\n",
    "\n",
    "rf_grid = {'max_depth': range(1,3)\n",
    "           'criterion': ['mse', 'mae'],\n",
    "           'min_samples_split' : range(2,5),\n",
    "           'min_samples_leaf' : range(2,5),\n",
    "           'n_estimators': range(5,10,1)\n",
    "        }\n",
    "xg_grid = {'max_depth': range(1,3)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings(record=True):\n",
    "    knn_rg=make_pipeline(minmax, GridSearchCV(knn, param_grid=knn_grid, cv = inner_cv, scoring=scoring))\n",
    "    svm_rg=make_pipeline(minmax, GridSearchCV(svm, param_grid=svm_grid, cv = inner_cv, scoring=scoring))\n",
    "    lr_rg=make_pipeline(std, GridSearchCV(lr, param_grid=lr_grid, cv = inner_cv, scoring=scoring))\n",
    "    dt_rg=GridSearchCV(dt, param_grid=dt_grid, cv = inner_cv, scoring=scoring)\n",
    "    rf_rg=GridSearchCV(rf, param_grid=rf_grid, cv = inner_cv, scoring=scoring)\n",
    "    xg_rg=GridSearchCV(xg, param_grid=xg_grid, cv = inner_cv, scoring=scoring)\n",
    "\n",
    "\n",
    "    knn_score=cross_val_score(knn_rg, X=X_train_trans2, y=y_train) \n",
    "    svm_score=cross_val_score(svm_rg, X=X_train_trans2, y=y_train)\n",
    "    lr_score=cross_val_score(lr_rg, X=X_train_trans2, y=y_train)\n",
    "    dt_score=cross_val_score(dt_rg, X=X_train_trans2, y=y_train)\n",
    "    rf_score=cross_val_score(rf_rg, X=X_train_trans2, y=y_train)\n",
    "    xg_score=cross_val_score(xg_rg, X=X_train_trans2, y=y_train)\n",
    "   \n",
    "\n",
    "    print((knn_score.mean()))\n",
    "    print((svm_score.mean())) \n",
    "    print((lr_score.mean())) \n",
    "    print((dt_score.mean())) \n",
    "    print((rf_score.mean())) \n",
    "    print((xg_score.mean())) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
